{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q crewai crewai-tools chromadb sentence-transformers 'crewai[tools]'\n",
        "!pip install -q langchain langchain-community langchain-huggingface\n",
        "!pip install -q accelerate transformers\n",
        "!pip install -q google-search-results python-dotenv\n",
        "!pip install -q requests beautifulsoup4\n",
        "!pip install -q litellm google-generativeai"
      ],
      "metadata": {
        "id": "jtGvsjA_o6Vc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import json\n",
        "from typing import List, Dict, Optional, Any\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "\n",
        "# CrewAI imports\n",
        "from crewai import Agent, Task, Crew, Process, LLM\n",
        "\n",
        "# âœ… CORRECT: Import tools from crewai_tools\n",
        "from crewai_tools import (\n",
        "    FileReadTool,\n",
        "    ScrapeWebsiteTool,\n",
        "    MDXSearchTool,\n",
        "    SerperDevTool\n",
        ")\n",
        "from crewai.tools import tool\n",
        "\n",
        "# Vector DB and embeddings\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# LangChain for LLM integration\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter # Changed import path\n",
        "\n",
        "# Web search\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"All libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNq06yUOr3lD",
        "outputId": "4740ebce-9c73-4ba6-9a1c-6736ee321075"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ENVIRONMENT SETUP\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get API keys from Colab secrets\n",
        "# To add secrets: Click icon in left sidebar > Add new secret\n",
        "try:\n",
        "    SERPER_API_KEY = userdata.get('SERPER_API_KEY')\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "except:\n",
        "    print(\"API keys not found in Colab secrets.\")\n",
        "    print(\"Please add SERPER_API_KEY and GEMINI_API_KEY in Colab secrets.\")\n",
        "    print(\"\\nFor now, enter them manually:\")\n",
        "    SERPER_API_KEY = input(\"Enter SERPER_API_KEY: \")\n",
        "    GEMINI_API_KEY = input(\"Enter GEMINI_API_KEY: \")\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['SERPER_API_KEY'] = SERPER_API_KEY\n",
        "os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY  # Set Gemini key\n",
        "\n",
        "print(\"Environment configured!\")\n",
        "print(f\"   - Serper API: {'âœ“' if SERPER_API_KEY else 'âœ—'}\")\n",
        "print(f\"   - Gemini API: {'âœ“' if GEMINI_API_KEY else 'âœ—'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktRzd8lbpRcJ",
        "outputId": "eb956d4b-97fa-4271-a698-1005e9c23b17"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Environment configured!\n",
            "   - Serper API: âœ“\n",
            "   - Gemini API: âœ“\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONTEXT ENGINEERING MODULE\n",
        "\n",
        "\n",
        "class ContextEngineeringModule:\n",
        "    \"\"\"\n",
        "    Advanced context management system with:\n",
        "    - Short-term memory (rolling window buffer)\n",
        "    - Long-term memory (vector database)\n",
        "    - Semantic retrieval and ranking\n",
        "    - Automatic context compression\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 short_term_size: int = 10,\n",
        "                 embedding_model: str = 'all-MiniLM-L6-v2',\n",
        "                 collection_name: str = 'agent_memory'):\n",
        "\n",
        "        print(\"ðŸ”§ Initializing Context Engineering Module...\")\n",
        "\n",
        "        # SHORT-TERM MEMORY: Rolling window for recent interactions\n",
        "        self.short_term_memory = deque(maxlen=short_term_size)\n",
        "        print(f\"   âœ“ Short-term memory: {short_term_size} message buffer\")\n",
        "\n",
        "        # LONG-TERM MEMORY: Vector database for semantic search\n",
        "        self.chroma_client = chromadb.Client(Settings(\n",
        "            anonymized_telemetry=False,\n",
        "            allow_reset=True\n",
        "        ))\n",
        "\n",
        "        # Reset collection if exists\n",
        "        try:\n",
        "            self.chroma_client.delete_collection(collection_name)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        self.collection = self.chroma_client.create_collection(\n",
        "            name=collection_name,\n",
        "            metadata={\"hnsw:space\": \"cosine\"}\n",
        "        )\n",
        "        print(f\"   âœ“ Long-term memory: ChromaDB '{collection_name}'\")\n",
        "\n",
        "        # EMBEDDING MODEL: For semantic similarity\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        print(f\"   âœ“ Embedding model: {embedding_model}\")\n",
        "\n",
        "        # MEMORY COUNTER: For unique IDs\n",
        "        self.memory_counter = 0\n",
        "\n",
        "        print(\"Context Engineering Module ready!\\n\")\n",
        "\n",
        "    def add_to_short_term(self, message: Dict[str, str]):\n",
        "        \"\"\"Add message to short-term memory (auto-expires old messages)\"\"\"\n",
        "        message['timestamp'] = datetime.now().isoformat()\n",
        "        self.short_term_memory.append(message)\n",
        "\n",
        "    def add_to_long_term(self, text: str, metadata: Optional[Dict] = None):\n",
        "        \"\"\"Store important information in long-term vector memory\"\"\"\n",
        "        if not text.strip():\n",
        "            return\n",
        "\n",
        "        # Generate embedding\n",
        "        embedding = self.embedding_model.encode(text).tolist()\n",
        "\n",
        "        # Store in vector DB\n",
        "        self.memory_counter += 1\n",
        "        self.collection.add(\n",
        "            embeddings=[embedding],\n",
        "            documents=[text],\n",
        "            metadatas=[metadata or {}],\n",
        "            ids=[f\"mem_{self.memory_counter}\"]\n",
        "        )\n",
        "\n",
        "    def retrieve_relevant_context(self, query: str, top_k: int = 5) -> List[str]:\n",
        "        \"\"\"Retrieve most relevant memories using semantic search\"\"\"\n",
        "        if self.collection.count() == 0:\n",
        "            return []\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = self.embedding_model.encode(query).tolist()\n",
        "\n",
        "        # Search vector database\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[query_embedding],\n",
        "            n_results=min(top_k, self.collection.count())\n",
        "        )\n",
        "\n",
        "        return results['documents'][0] if results['documents'] else []\n",
        "\n",
        "    def build_context_prompt(self,\n",
        "                            user_query: str,\n",
        "                            system_prompt: str = \"\",\n",
        "                            include_short_term: bool = True,\n",
        "                            include_long_term: bool = True) -> str:\n",
        "        \"\"\"Construct complete context for agent with all relevant information\"\"\"\n",
        "\n",
        "        context_parts = []\n",
        "\n",
        "        # Add system prompt\n",
        "        if system_prompt:\n",
        "            context_parts.append(f\"SYSTEM: {system_prompt}\")\n",
        "\n",
        "        # Add long-term memory (most relevant historical context)\n",
        "        if include_long_term:\n",
        "            relevant_memories = self.retrieve_relevant_context(user_query, top_k=3)\n",
        "            if relevant_memories:\n",
        "                context_parts.append(\"\\nRELEVANT KNOWLEDGE:\")\n",
        "                for i, mem in enumerate(relevant_memories, 1):\n",
        "                    context_parts.append(f\"{i}. {mem}\")\n",
        "\n",
        "        # Add short-term memory (recent conversation)\n",
        "        if include_short_term and self.short_term_memory:\n",
        "            context_parts.append(\"\\nRECENT CONVERSATION:\")\n",
        "            for msg in list(self.short_term_memory)[-5:]:\n",
        "                role = msg.get('role', 'unknown')\n",
        "                content = msg.get('content', '')\n",
        "                context_parts.append(f\"{role.upper()}: {content}\")\n",
        "\n",
        "        # Add current query\n",
        "        context_parts.append(f\"\\nCURRENT QUERY: {user_query}\")\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def compress_context(self, text: str, max_length: int = 500) -> str:\n",
        "        \"\"\"Compress long context using extractive summarization\"\"\"\n",
        "        if len(text) <= max_length:\n",
        "            return text\n",
        "\n",
        "        # Simple compression: take first and last portions\n",
        "        half = max_length // 2\n",
        "        return f\"{text[:half]}... [TRUNCATED] ...{text[-half:]}\"\n",
        "\n",
        "    def get_memory_stats(self) -> Dict:\n",
        "        \"\"\"Get current memory statistics\"\"\"\n",
        "        return {\n",
        "            'short_term_messages': len(self.short_term_memory),\n",
        "            'long_term_items': self.collection.count(),\n",
        "            'total_memories': self.memory_counter\n",
        "        }\n",
        "\n",
        "# Initialize the context module\n",
        "context_module = ContextEngineeringModule(short_term_size=10)\n",
        "print(f\"Memory Stats: {context_module.get_memory_stats()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3-A7wl9pf2M",
        "outputId": "0f839b92-216d-4429-ef96-6a7b18fe11bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”§ Initializing Context Engineering Module...\n",
            "   âœ“ Short-term memory: 10 message buffer\n",
            "   âœ“ Long-term memory: ChromaDB 'agent_memory'\n",
            "   âœ“ Embedding model: all-MiniLM-L6-v2\n",
            "âœ… Context Engineering Module ready!\n",
            "\n",
            "ðŸ“Š Memory Stats: {'short_term_messages': 0, 'long_term_items': 0, 'total_memories': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_enhanced_search_function(serper_tool, context_module):\n",
        "    \"\"\"\n",
        "    Creates a simpler function-based search tool.\n",
        "    This is the RECOMMENDED approach as it's more reliable with CrewAI.\n",
        "    \"\"\"\n",
        "    from crewai.tools import tool\n",
        "\n",
        "    @tool(\"enhanced_web_search\")\n",
        "    def enhanced_web_search(query: str) -> str:\n",
        "        \"\"\"\n",
        "        Enhanced web search that integrates with the memory system.\n",
        "        Uses SerperDevTool and stores results in long-term memory.\n",
        "\n",
        "        Args:\n",
        "            query: The search query string\n",
        "\n",
        "        Returns:\n",
        "            Formatted search results with memory integration\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(f\"ðŸ” Executing search for: {query}\")\n",
        "\n",
        "            # âœ… CORRECT: Use search_query keyword argument\n",
        "            results = serper_tool._run(search_query=query)\n",
        "\n",
        "            # Handle dict response from Serper\n",
        "            if isinstance(results, dict):\n",
        "                formatted_results = []\n",
        "\n",
        "                # Extract answer box\n",
        "                if 'answerBox' in results:\n",
        "                    answer = results['answerBox']\n",
        "                    answer_text = answer.get('answer', answer.get('snippet', ''))\n",
        "                    if answer_text:\n",
        "                        formatted_results.append(f\"Quick Answer: {answer_text}\\n\")\n",
        "\n",
        "                # Extract knowledge graph\n",
        "                if 'knowledgeGraph' in results:\n",
        "                    kg = results['knowledgeGraph']\n",
        "                    title = kg.get('title', '')\n",
        "                    desc = kg.get('description', '')\n",
        "                    if title or desc:\n",
        "                        formatted_results.append(f\"ðŸ“š Knowledge Graph: {title} - {desc}\\n\")\n",
        "\n",
        "                # Extract organic results\n",
        "                if 'organic' in results:\n",
        "                    formatted_results.append(\"ðŸ” Top Search Results:\\n\")\n",
        "                    for i, item in enumerate(results['organic'][:5], 1):\n",
        "                        title = item.get('title', 'No title')\n",
        "                        link = item.get('link', 'No link')\n",
        "                        snippet = item.get('snippet', 'No description')\n",
        "                        formatted_results.append(f\"{i}. {title}\")\n",
        "                        formatted_results.append(f\"   URL: {link}\")\n",
        "                        formatted_results.append(f\"   {snippet}\\n\")\n",
        "\n",
        "                results_str = \"\\n\".join(formatted_results) if formatted_results else str(results)\n",
        "            else:\n",
        "                results_str = str(results)\n",
        "\n",
        "            if not results_str or results_str.strip() == \"\":\n",
        "                return f\"No results found for query: {query}\"\n",
        "\n",
        "            # Store in long-term memory for RAG\n",
        "            context_module.add_to_long_term(\n",
        "                f\"Search Query: {query}\\nResults: {results_str}\",\n",
        "                metadata={\n",
        "                    'source': 'web_search',\n",
        "                    'query': query,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "            )\n",
        "\n",
        "            return f\"ðŸ” Search Results for: '{query}'\\n\\n{results_str}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Search error: {str(e)}\"\n",
        "            print(f\"{error_msg}\")\n",
        "            print(f\"   Error type: {type(e).__name__}\")\n",
        "            return f\"Search failed: {error_msg}\"\n",
        "\n",
        "    return enhanced_web_search"
      ],
      "metadata": {
        "id": "zTLkAAEP_6pW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fbEEh9ja_753"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initializing CrewAI Tools...\\n\")\n",
        "\n",
        "try:\n",
        "    # 1. SerperDevTool - Web Search\n",
        "    serper_tool = SerperDevTool()\n",
        "    print(\"SerperDevTool initialized (Web Search)\")\n",
        "\n",
        "    # 2. ScrapeWebsiteTool - Website Scraping\n",
        "    scrape_tool = ScrapeWebsiteTool()\n",
        "    print(\"ScrapeWebsiteTool initialized (Website Scraping)\")\n",
        "\n",
        "    # 3. Enhanced Web Search Tool with robust input handling\n",
        "    enhanced_web_search_tool = create_enhanced_search_function(serper_tool, context_module)\n",
        "    print(\"Enhanced Web Search Tool initialized (Robust Input Handling)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing tools: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\n",
        "print(\"\\nAll tools ready!\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHp8AQXPwPWM",
        "outputId": "7ba3f4e1-4c1a-4d48-e781-0dbf3d4f2c92"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”§ Initializing CrewAI Tools...\n",
            "\n",
            "âœ“ SerperDevTool initialized (Web Search)\n",
            "âœ“ ScrapeWebsiteTool initialized (Website Scraping)\n",
            "âœ“ Enhanced Web Search Tool initialized (Robust Input Handling)\n",
            "\n",
            "âœ… All tools ready!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: LLM CONFIGURATION\n",
        "\n",
        "\n",
        "def create_llm(model_choice: str = \"gemini\"):\n",
        "    \"\"\"\n",
        "    Create LLM instance using CrewAI's LLM class.\n",
        "\n",
        "    Available models:\n",
        "    - gemini: Google Gemini (FREE, recommended)\n",
        "    - gemini-flash: Faster Gemini model\n",
        "    - gemini-pro: More capable Gemini model\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        if model_choice == \"gemini\":\n",
        "            print(\"ðŸ”§ Initializing Gemini 1.5 Flash (Fast & Free)...\")\n",
        "            llm = LLM(\n",
        "                model=\"gemini-2.5-flash-lite\",\n",
        "                temperature=0.7,\n",
        "                api_key=os.environ.get('GOOGLE_API_KEY'),\n",
        "                max_retries=5,\n",
        "                timeout=120,\n",
        "                max_tokens=2048,\n",
        "                request_timeout=60\n",
        "\n",
        "            )\n",
        "            print(\"LLM initialized: Gemini 1.5 Flash\")\n",
        "\n",
        "        elif model_choice == \"gemini-pro\":\n",
        "            print(\"Initializing Gemini 1.5 Pro (More Capable)...\")\n",
        "            llm = LLM(\n",
        "                model=\"gemini/gemini-1.5-pro\",\n",
        "                temperature=0.7,\n",
        "                api_key=os.environ.get('GOOGLE_API_KEY')\n",
        "            )\n",
        "            print(\"LLM initialized: Gemini 1.5 Pro\")\n",
        "\n",
        "        else:\n",
        "            print(\"Initializing default Gemini model...\")\n",
        "            llm = LLM(\n",
        "                model=\"gemini/gemini-pro\",\n",
        "                temperature=0.7,\n",
        "                api_key=os.environ.get('GOOGLE_API_KEY')\n",
        "            )\n",
        "            print(\"LLM initialized: Gemini Pro\")\n",
        "\n",
        "        return llm\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing LLM: {e}\")\n",
        "        print(\"   Please check your GEMINI_API_KEY\")\n",
        "        raise\n",
        "\n",
        "# Initialize LLM with Gemini\n",
        "# Options: \"gemini\" (flash), \"gemini-pro\", or default\n",
        "llm = create_llm(\"gemini\")  # Using Gemini 1.5 Flash (fast & free)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmXyvBBEyBmD",
        "outputId": "ebc4801e-1393-478b-830b-3b528f26912f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”§ Initializing Gemini 1.5 Flash (Fast & Free)...\n",
            "âœ… LLM initialized: Gemini 1.5 Flash\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MULTI-AGENT ARCHITECTURE\n",
        "\n",
        "\n",
        "print(\"\\nBuilding Multi-Agent System...\\n\")\n",
        "\n",
        "# AGENT 1: RESEARCH AGENT\n",
        "research_agent = Agent(\n",
        "    role='Research Specialist',\n",
        "    goal='Search and retrieve the most relevant and accurate information from the web',\n",
        "    backstory=\"\"\"You are an expert research analyst with exceptional skills in:\n",
        "    - Formulating effective search queries\n",
        "    - Evaluating source credibility and relevance\n",
        "    - Extracting key information from multiple sources\n",
        "    - Identifying patterns and connections across documents\n",
        "    - Maintaining objectivity and avoiding bias\n",
        "\n",
        "    You always prioritize accuracy and cite your sources clearly.\"\"\",\n",
        "    tools=[enhanced_web_search_tool],\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    max_iter=1,  # REDUCED from 3 to 1 to save API calls\n",
        "    max_rpm=10\n",
        ")\n",
        "print(\"Research Agent created (powered by Gemini)\")\n",
        "\n",
        "# AGENT 2: ANALYSIS AGENT\n",
        "analysis_agent = Agent(\n",
        "    role='Critical Analysis Expert',\n",
        "    goal='Synthesize information, detect contradictions, and provide deep analytical insights',\n",
        "    backstory=\"\"\"You are a brilliant analyst specializing in:\n",
        "    - Critical thinking and logical reasoning\n",
        "    - Identifying contradictions and inconsistencies\n",
        "    - Synthesizing complex information into clear insights\n",
        "    - Cross-referencing multiple sources for accuracy\n",
        "    - Detecting potential biases or hallucinations\n",
        "\n",
        "    You never make unfounded claims and always acknowledge uncertainty when appropriate.\"\"\",\n",
        "    tools=[],\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    max_iter=1,  # REDUCED from 3 to 1 to save API calls\n",
        "    max_rpm=10\n",
        ")\n",
        "print(\"Analysis Agent created (powered by Gemini)\")\n",
        "\n",
        "# AGENT 3: WRITING AGENT\n",
        "writing_agent = Agent(\n",
        "    role='Professional Content Writer',\n",
        "    goal='Transform analyzed information into clear, accurate, and engaging final output',\n",
        "    backstory=\"\"\"You are an expert technical writer with skills in:\n",
        "    - Clear and concise communication\n",
        "    - Maintaining factual accuracy while being engaging\n",
        "    - Structuring information logically\n",
        "    - Adapting tone and style to the audience\n",
        "    - Ensuring context integrity and coherence\n",
        "\n",
        "    You transform complex analyses into accessible, well-structured responses.\"\"\",\n",
        "    tools=[],\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    max_iter=1,  # REDUCED from 3 to 1 to save API calls\n",
        "    max_rpm=10\n",
        ")\n",
        "print(\"Writing Agent created (powered by Gemini)\\n\")\n",
        "\n",
        "print(\"All agents initialized successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0bS5uxOyKRP",
        "outputId": "03ca61af-2788-4a7b-e0de-07a271bb509e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ—‚ï¸ Building Multi-Agent System...\n",
            "\n",
            "âœ“ Research Agent created (powered by Gemini)\n",
            "âœ“ Analysis Agent created (powered by Gemini)\n",
            "âœ“ Writing Agent created (powered by Gemini)\n",
            "\n",
            "âœ… All agents initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_agent_tasks(user_query: str, context_summary: str, research_agent, analysis_agent, writing_agent):\n",
        "    \"\"\"\n",
        "    Creates properly configured tasks that prevent the analysis agent from rejecting valid research.\n",
        "    \"\"\"\n",
        "\n",
        "    # TASK 1: RESEARCH TASK\n",
        "    research_task = Task(\n",
        "        description=f\"\"\"CRITICAL: You MUST use the enhanced_web_search tool to find REAL information.\n",
        "\n",
        "        Query: {user_query}\n",
        "\n",
        "        Context from memory: {context_summary}\n",
        "\n",
        "        MANDATORY STEPS:\n",
        "        1. Call enhanced_web_search tool with the query\n",
        "        2. Wait for REAL results from the tool\n",
        "        3. Extract and summarize the actual search results\n",
        "        4. DO NOT fabricate or simulate any data\n",
        "        5. If tool fails, report the error explicitly\n",
        "\n",
        "        FORBIDDEN:\n",
        "        - DO NOT use internal knowledge without tool verification\n",
        "        - DO NOT make up search results\n",
        "        - DO NOT proceed without calling the tool\n",
        "        \"\"\",\n",
        "        agent=research_agent,\n",
        "        expected_output=\"Real search results from the enhanced_web_search tool with source citations, URLs, and content snippets\"\n",
        "    )\n",
        "\n",
        "    # TASK 2: ANALYSIS TASK\n",
        "    analysis_task = Task(\n",
        "        description=f\"\"\"You are analyzing web search results provided by the Research Specialist.\n",
        "\n",
        "        IMPORTANT CLARIFICATIONS:\n",
        "        - \"Verified research data\" = Web search summaries, URLs, and snippets from the previous agent\n",
        "        - You ARE NOT expecting: academic papers, raw datasets, or scientific studies\n",
        "        - You ARE expecting: Search engine results with titles, URLs, and text snippets\n",
        "        - The previous agent's output IS the research data - do not claim it's missing\n",
        "\n",
        "        YOUR JOB:\n",
        "        1. READ the search results provided by the Research Specialist above\n",
        "        2. EXTRACT key information from the titles, URLs, and snippets\n",
        "        3. SYNTHESIZE the main points into a coherent analysis\n",
        "        4. IDENTIFY patterns or themes across multiple search results\n",
        "        5. NOTE any contradictions between different sources\n",
        "        6. FLAG genuine limitations (e.g., \"sources only cover X, not Y\")\n",
        "\n",
        "        FORBIDDEN BEHAVIORS:\n",
        "        - DO NOT claim \"no data was provided\" when search results are present\n",
        "        - DO NOT reject URLs/snippets as \"not real research data\"\n",
        "        - DO NOT demand academic papers or datasets\n",
        "        - DO NOT hallucinate that information is missing when it's clearly there\n",
        "        - DO NOT add information from your own knowledge - use only what's provided\n",
        "\n",
        "        ACCEPTABLE LIMITATIONS TO NOTE:\n",
        "        - \"Search results are limited to top 5 matches\"\n",
        "        - \"No specific statistics were found, only general descriptions\"\n",
        "        - \"Sources focus on X aspect but don't cover Y aspect\"\n",
        "\n",
        "        Query being analyzed: {user_query}\n",
        "\n",
        "        Now analyze the search results provided by the Research Specialist above.\n",
        "        \"\"\",\n",
        "        agent=analysis_agent,\n",
        "        expected_output=\"\"\"A synthesis of the web search results that:\n",
        "        - Summarizes key information from the titles, URLs, and snippets\n",
        "        - Identifies main themes or patterns\n",
        "        - Notes any contradictions between sources\n",
        "        - Acknowledges actual limitations (not fabricated ones)\n",
        "        - DOES NOT claim data is missing when search results were provided\"\"\",\n",
        "        context=[research_task]\n",
        "    )\n",
        "\n",
        "    # TASK 3: WRITING TASK\n",
        "    writing_task = Task(\n",
        "        description=f\"\"\"Create a comprehensive answer based on the research and analysis provided above.\n",
        "\n",
        "        Query: {user_query}\n",
        "\n",
        "        YOUR SOURCES:\n",
        "        1. Research Specialist's web search results (titles, URLs, snippets)\n",
        "        2. Analysis Expert's synthesis of those results\n",
        "\n",
        "        REQUIREMENTS:\n",
        "        1. Use ONLY information from the research and analysis above\n",
        "        2. Structure the answer logically with clear sections\n",
        "        3. Include specific details from the search results\n",
        "        4. Cite sources by mentioning URLs or source names when relevant\n",
        "        5. If the analysis noted limitations, acknowledge them briefly\n",
        "        6. DO NOT add external information from your own knowledge\n",
        "        7. DO NOT claim information is missing if it was provided in earlier steps\n",
        "\n",
        "        FORMAT:\n",
        "        - Start with a direct answer to the query\n",
        "        - Provide supporting details from the search results\n",
        "        - Include 2-3 relevant source URLs if available\n",
        "        - End with a brief summary or key takeaway\n",
        "\n",
        "        Tone: Clear, informative, and factual\n",
        "        \"\"\",\n",
        "        agent=writing_agent,\n",
        "        expected_output=\"\"\"A well-structured answer that:\n",
        "        - Directly addresses the user's query\n",
        "        - Uses specific information from the web search results\n",
        "        - Includes source citations (URLs)\n",
        "        - Is accurate and based solely on provided research\n",
        "        - Acknowledges real limitations if they exist\"\"\",\n",
        "        context=[research_task, analysis_task]\n",
        "    )\n",
        "\n",
        "    return research_task, analysis_task, writing_task\n"
      ],
      "metadata": {
        "id": "F7BfB1SyDOoz"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(user_query: str, use_memory: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Complete pipeline with fixed task definitions.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STARTING MULTI-AGENT PIPELINE\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    try:\n",
        "        # STEP 1: CONTEXT ENGINEERING\n",
        "        print(\"Step 1: Context Retrieval...\")\n",
        "\n",
        "        if use_memory:\n",
        "            relevant_context = context_module.retrieve_relevant_context(user_query, top_k=3)\n",
        "            context_summary = \"\\n\".join(relevant_context) if relevant_context else \"No relevant historical context.\"\n",
        "            print(f\"   Retrieved {len(relevant_context)} relevant memories\")\n",
        "        else:\n",
        "            context_summary = \"No historical context used.\"\n",
        "            print(\"   Skipping memory retrieval\")\n",
        "\n",
        "        context_module.add_to_short_term({\n",
        "            'role': 'user',\n",
        "            'content': user_query\n",
        "        })\n",
        "\n",
        "        # STEP 2: CREATE AGENT TASKS WITH FIXED DEFINITIONS\n",
        "        print(\"\\nStep 2: Creating Agent Tasks with Fixed Logic...\")\n",
        "\n",
        "        research_task, analysis_task, writing_task = create_agent_tasks(\n",
        "            user_query=user_query,\n",
        "            context_summary=context_summary,\n",
        "            research_agent=research_agent,\n",
        "            analysis_agent=analysis_agent,\n",
        "            writing_agent=writing_agent\n",
        "        )\n",
        "\n",
        "        # STEP 3: CREATE AND RUN CREW\n",
        "        print(\"\\n Step 3: Assembling Crew...\")\n",
        "\n",
        "        crew = Crew(\n",
        "            agents=[research_agent, analysis_agent, writing_agent],\n",
        "            tasks=[research_task, analysis_task, writing_task],\n",
        "            process=Process.sequential,\n",
        "            verbose=True,\n",
        "            max_rpm=10,\n",
        "            share_crew=False\n",
        "        )\n",
        "\n",
        "        print(\"\\nStep 4: Executing Multi-Agent Workflow...\\n\")\n",
        "\n",
        "        # Execute\n",
        "        result = crew.kickoff()\n",
        "        final_answer = str(result)\n",
        "\n",
        "        # UPDATE MEMORY\n",
        "        print(\"\\nStep 5: Updating Memory...\")\n",
        "\n",
        "        context_module.add_to_short_term({\n",
        "            'role': 'assistant',\n",
        "            'content': final_answer\n",
        "        })\n",
        "\n",
        "        context_module.add_to_long_term(\n",
        "            f\"Query: {user_query}\\nAnswer: {final_answer}\",\n",
        "            metadata={'type': 'qa_pair', 'timestamp': datetime.now().isoformat()}\n",
        "        )\n",
        "\n",
        "        stats = context_module.get_memory_stats()\n",
        "        print(f\"   Short-term: {stats['short_term_messages']} messages\")\n",
        "        print(f\"   Long-term: {stats['long_term_items']} items stored\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"PIPELINE COMPLETE\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "        return final_answer\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in pipeline execution: {str(e)}\"\n",
        "        print(f\"\\n{error_msg}\")\n",
        "        print(f\"Error type: {type(e).__name__}\")\n",
        "\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        return f\"Pipeline failed: {error_msg}\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BdkSUT_zDWX9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_test_query(query: str):\n",
        "    \"\"\"Wrapper to safely test queries with error handling.\"\"\"\n",
        "    try:\n",
        "        print(\"\\n\" + \"#\"*70)\n",
        "        print(f\"# TEST QUERY: {query}\")\n",
        "        print(\"#\"*70)\n",
        "\n",
        "        answer = process_query(query)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FINAL ANSWER:\")\n",
        "        print(\"=\"*70)\n",
        "        print(answer)\n",
        "\n",
        "        # Wait between queries to respect rate limits\n",
        "        print(\"\\nâ±ï¸  Waiting 10 seconds before next query...\")\n",
        "        time.sleep(10)\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nQuery interrupted by user\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTest query failed: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "22gB7YK1zFiB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = safe_test_query(\"What is CrewAI?\")"
      ],
      "metadata": {
        "id": "2nsXgTZ7zGXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Technical Explanation\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"# EXAMPLE 1: Retrieval-Augmented Generation (RAG)\")\n",
        "print(\"#\"*70)\n",
        "\n",
        "answer1 = process_query(\n",
        "    \"Explain retrieval-augmented generation (RAG) and how it improves LLM responses\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL ANSWER:\")\n",
        "print(\"=\"*70)\n",
        "print(answer1)"
      ],
      "metadata": {
        "id": "tCWkcPtM33N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Example 2: Current Research\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"# EXAMPLE 2: Multi-Agent Systems Research\")\n",
        "print(\"#\"*70)\n",
        "\n",
        "answer2 = process_query(\n",
        "    \"What are the latest developments in multi-agent AI systems?\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL ANSWER:\")\n",
        "print(\"=\"*70)\n",
        "print(answer2)\n",
        "\n",
        "# Example 3: Concept Definition\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"# EXAMPLE 3: Context Engineering\")\n",
        "print(\"#\"*70)\n",
        "\n",
        "answer3 = process_query(\n",
        "    \"What is context engineering in AI systems and why is it important?\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL ANSWER:\")\n",
        "print(\"=\"*70)\n",
        "print(answer3)\n",
        "\n",
        "# Example 4: Framework Comparison\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"# EXAMPLE 4: CrewAI vs AutoGen\")\n",
        "print(\"#\"*70)\n",
        "\n",
        "answer4 = process_query(\n",
        "    \"Compare CrewAI and AutoGen frameworks for building multi-agent systems\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL ANSWER:\")\n",
        "print(\"=\"*70)\n",
        "print(answer4)\n",
        "\n",
        "# Example 5: Technical Architecture\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"# EXAMPLE 5: LLM Memory Architectures\")\n",
        "print(\"#\"*70)\n",
        "\n",
        "answer5 = process_query(\n",
        "    \"Describe different memory architectures used in Large Language Models\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL ANSWER:\")\n",
        "print(\"=\"*70)\n",
        "print(answer5)\n"
      ],
      "metadata": {
        "id": "lUrOAmYR2moA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-DslIfu_o17n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d33b38-2bf2-4998-d4a6-72fe7d71d731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "MEMORY SYSTEM STATISTICS\n",
            "======================================================================\n",
            "\n",
            "Short-Term Memory:\n",
            "  â””â”€ Current Messages: 8/10\n",
            "\n",
            "Long-Term Memory (Vector DB):\n",
            "  â”œâ”€ Total Items Stored: 6\n",
            "  â””â”€ Total Memories Created: 6\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Recent Conversation History:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "1. [ASSISTANT] 2025-11-30T04:20:18.364643\n",
            "   Based on the provided context, no verified research data or search results regarding \"CrewAI\" were s...\n",
            "\n",
            "2. [USER] 2025-11-30T04:23:28.175175\n",
            "   What is CrewAI?\n",
            "\n",
            "3. [ASSISTANT] 2025-11-30T04:23:34.851052\n",
            "   Based on the provided context, CrewAI is described as a leading multi-agent platform and a standalon...\n",
            "\n",
            "4. [USER] 2025-11-30T04:42:31.053389\n",
            "   What is CrewAI?\n",
            "\n",
            "5. [ASSISTANT] 2025-11-30T04:42:44.870108\n",
            "   CrewAI is an open-source, Python-based framework designed for orchestrating autonomous AI agents. It...\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ================================================================================\n",
        "# STEP 10: MEMORY VISUALIZATION AND STATS\n",
        "# ================================================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Get memory statistics\n",
        "stats = context_module.get_memory_stats()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MEMORY SYSTEM STATISTICS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(f\"Short-Term Memory:\")\n",
        "print(f\"  â””â”€ Current Messages: {stats['short_term_messages']}/{context_module.short_term_memory.maxlen}\")\n",
        "print(f\"\\nLong-Term Memory (Vector DB):\")\n",
        "print(f\"  â”œâ”€ Total Items Stored: {stats['long_term_items']}\")\n",
        "print(f\"  â””â”€ Total Memories Created: {stats['total_memories']}\")\n",
        "\n",
        "# Display recent short-term memory\n",
        "if context_module.short_term_memory:\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"Recent Conversation History:\")\n",
        "    print(\"-\"*70 + \"\\n\")\n",
        "\n",
        "    for i, msg in enumerate(list(context_module.short_term_memory)[-5:], 1):\n",
        "        role = msg.get('role', 'unknown').upper()\n",
        "        content = msg.get('content', '')[:100] + \"...\" if len(msg.get('content', '')) > 100 else msg.get('content', '')\n",
        "        timestamp = msg.get('timestamp', 'N/A')\n",
        "        print(f\"{i}. [{role}] {timestamp}\")\n",
        "        print(f\"   {content}\\n\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "\n"
      ]
    }
  ]
}